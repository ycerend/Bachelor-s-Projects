{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CMPE 462 - Project 2<br>Implementing an SVM Classifier<br>Due: May 18, 2020, 23:59</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Student ID1:** 2016400249\n",
    "* **Student ID2:** 2014400063\n",
    "* **Student ID3:** 2009400165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, you are going to implement SVM. For this purpose, a data set (data.mat) is given to you. You can load the mat dataset into Python using the function `loadmat` in `Scipy.io`. When you load the data, you will obtain a dictionary object, where `X` stores the data matrix and `Y` stores the labels. You can use the first 150 samples for training and the rest for testing. In this project, you will use the software package [`LIBSVM`](http://www.csie.ntu.edu.tw/~cjlin/libsvm/) to implement SVM. Note that `LIBSVM` has a [`Python interface`](https://github.com/cjlin1/libsvm/tree/master/python), so you can call the SVM functions in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "#loading the data\n",
    "dataset = scipy.io.loadmat('data.mat')\n",
    "\n",
    "#converting data matrix X and labels Y into numpy arrays\n",
    "data_matrix =  np.array(dataset['X'])\n",
    "data_labels = np.array(dataset['Y']).flatten()\n",
    "\n",
    "#dividing data as 150 samples for training and the rest for the test \n",
    "training_set_x,test_set_x = data_matrix[:150], data_matrix[150:]\n",
    "training_set_labels, test_set_labels = data_labels[:150],data_labels[150:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - 30 pts\n",
    "\n",
    "Train a hard margin linear SVM and report both train and test classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#importing the library path to the system\n",
    "sys.path.append('./libsvm-3.24/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 74.6667% (112/150) (classification)\n",
      "Accuracy = 77.5% (93/120) (classification)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "from svmutil import *\n",
    "\n",
    "# We need a very high C value to approximate hard margine SVM.\n",
    "# Training accuracy is not 100% because the data is not linearly seperable.\n",
    "# We see that this actually is the solution with the optimal margin because\n",
    "# test accuracy does not change with C is increasing.\n",
    "\n",
    "hardMarginSvm = svm_train(training_set_labels,training_set_x, '-t 0 -c 420420420420')\n",
    "\n",
    "p_label_train, p_acc_train, p_val_train = svm_predict(training_set_labels,training_set_x, hardMarginSvm)\n",
    "p_label_test, p_acc_test, p_val_test = svm_predict(test_set_labels, test_set_x, hardMarginSvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - 40 pts\n",
    "\n",
    "Train soft margin SVM for different values of the parameter $C$, and with different kernel functions. Systematically report your results. For instance, report the performances of different kernels for a fixed $C$, then report the performance for different $C$ values for a fixed kernel, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.6667% (127/150) (classification)\n",
      "Accuracy = 84.1667% (101/120) (classification)\n",
      "Accuracy = 86.6667% (130/150) (classification)\n",
      "Accuracy = 85% (102/120) (classification)\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "Accuracy = 88.6667% (133/150) (classification)\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "Accuracy = 90% (135/150) (classification)\n",
      "Accuracy = 81.6667% (98/120) (classification)\n",
      "Accuracy = 53.3333% (80/150) (classification)\n",
      "Accuracy = 58.3333% (70/120) (classification)\n",
      "Accuracy = 84.6667% (127/150) (classification)\n",
      "Accuracy = 79.1667% (95/120) (classification)\n",
      "Accuracy = 91.3333% (137/150) (classification)\n",
      "Accuracy = 80.8333% (97/120) (classification)\n",
      "Accuracy = 97.3333% (146/150) (classification)\n",
      "Accuracy = 80% (96/120) (classification)\n",
      "Accuracy = 100% (150/150) (classification)\n",
      "Accuracy = 75% (90/120) (classification)\n",
      "Accuracy = 76.6667% (115/150) (classification)\n",
      "Accuracy = 77.5% (93/120) (classification)\n",
      "Accuracy = 84.6667% (127/150) (classification)\n",
      "Accuracy = 83.3333% (100/120) (classification)\n",
      "Accuracy = 91.3333% (137/150) (classification)\n",
      "Accuracy = 80% (96/120) (classification)\n",
      "Accuracy = 98% (147/150) (classification)\n",
      "Accuracy = 77.5% (93/120) (classification)\n",
      "Accuracy = 100% (150/150) (classification)\n",
      "Accuracy = 77.5% (93/120) (classification)\n",
      "Accuracy = 79.3333% (119/150) (classification)\n",
      "Accuracy = 84.1667% (101/120) (classification)\n",
      "Accuracy = 84% (126/150) (classification)\n",
      "Accuracy = 84.1667% (101/120) (classification)\n",
      "Accuracy = 82.6667% (124/150) (classification)\n",
      "Accuracy = 82.5% (99/120) (classification)\n",
      "Accuracy = 77.3333% (116/150) (classification)\n",
      "Accuracy = 72.5% (87/120) (classification)\n",
      "Accuracy = 75.3333% (113/150) (classification)\n",
      "Accuracy = 74.1667% (89/120) (classification)\n",
      "[[ 84.66666667  53.33333333  76.66666667  79.33333333]\n",
      " [ 86.66666667  84.66666667  84.66666667  84.        ]\n",
      " [ 88.66666667  91.33333333  91.33333333  82.66666667]\n",
      " [ 88.66666667  97.33333333  98.          77.33333333]\n",
      " [ 90.         100.         100.          75.33333333]]\n",
      "[[84.16666667 58.33333333 77.5        84.16666667]\n",
      " [85.         79.16666667 83.33333333 84.16666667]\n",
      " [81.66666667 80.83333333 80.         82.5       ]\n",
      " [81.66666667 80.         77.5        72.5       ]\n",
      " [81.66666667 75.         77.5        74.16666667]]\n"
     ]
    }
   ],
   "source": [
    "# Experiments class is used for training soft margin svm with different  values of the parameter C \n",
    "# and with different kernel functions.\n",
    "class Experiments:\n",
    "    # Initializing training,test sets and its labels and also candidate values\n",
    "    def __init__(self, training_set_x, training_set_labels, test_set_labels, test_set_x,c_parameters,candidate_kernels):\n",
    "            self.training_set_x = training_set_x\n",
    "            self.training_set_labels =  training_set_labels\n",
    "            self.test_set_labels = test_set_labels\n",
    "            self.test_set_x = test_set_x\n",
    "            self.c_parameters = c_parameters\n",
    "            self.candidate_kernels = candidate_kernels\n",
    "            self.allResults = {}\n",
    "            self.testAccuracies=np.array([0.0]*(len(c_parameters)*len(candidate_kernels)))\n",
    "            self.testAccuracies=self.testAccuracies.reshape((len(c_parameters),len(candidate_kernels)))\n",
    "            self.trainAccuracies=np.array([0.0]*(len(c_parameters)*len(candidate_kernels)))\n",
    "            self.trainAccuracies=self.trainAccuracies.reshape((len(c_parameters),len(candidate_kernels)))\n",
    "    \n",
    "    #training svm with a given parameter specified in trainWithDifferentSetups fuction \n",
    "    def trainSvm(self, k_index,c_index):\n",
    "        parameter = '-t ' + str(self.candidate_kernels[k_index]) + ' -c ' + str(self.c_parameters[c_index])\n",
    "        svm = svm_train(self.training_set_labels,training_set_x, parameter)\n",
    "        p_label_train, p_acc_train, p_val_train = svm_predict(self.training_set_labels,self.training_set_x, svm)\n",
    "        p_label_test, p_acc_test, p_val_test = svm_predict(self.test_set_labels, self.test_set_x, svm)\n",
    "        \n",
    "        #appeding results of the tested svm\n",
    "        self.allResults[parameter] = {'training': (p_label_train, p_acc_train, p_val_train),'test' : (p_label_test, p_acc_test, p_val_test)}\n",
    "        self.testAccuracies[c_index,k_index]=p_acc_test[0]\n",
    "        self.trainAccuracies[c_index,k_index]=p_acc_train[0]\n",
    "        \n",
    "    #training with different different  values of the parameter C and with different kernel functions.\n",
    "    def trainWithDifferentSetups(self):\n",
    "        for k_index in range(0,len(self.candidate_kernels)):\n",
    "            for c_index in range(0,len(self.c_parameters)):\n",
    "                self.trainSvm(k_index,c_index )\n",
    "\n",
    "c_parameters = [0.05,0.5,5,50,500]\n",
    "candidate_kernels = [0,1,2,3]\n",
    "experiments =  Experiments(training_set_x, training_set_labels, test_set_labels, test_set_x,c_parameters,candidate_kernels)\n",
    "experiments.trainWithDifferentSetups()\n",
    "print(experiments.trainAccuracies)\n",
    "print(experiments.testAccuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - 15 pts\n",
    "\n",
    "Please report how the number of support vectors changes as the value of $C$ increases (while all other parameters remain the same). Discuss whether your observations match the theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:1\t # of support vectors:58\n",
      "C:2\t # of support vectors:56\n",
      "C:3\t # of support vectors:55\n",
      "C:4\t # of support vectors:54\n",
      "C:5\t # of support vectors:54\n",
      "C:6\t # of support vectors:53\n",
      "C:7\t # of support vectors:52\n",
      "C:8\t # of support vectors:52\n",
      "C:9\t # of support vectors:50\n",
      "C:10\t # of support vectors:51\n",
      "C:11\t # of support vectors:51\n",
      "C:12\t # of support vectors:51\n",
      "C:13\t # of support vectors:51\n",
      "C:14\t # of support vectors:50\n",
      "C:15\t # of support vectors:50\n",
      "C:16\t # of support vectors:50\n",
      "C:17\t # of support vectors:50\n",
      "C:18\t # of support vectors:50\n",
      "C:19\t # of support vectors:50\n"
     ]
    }
   ],
   "source": [
    "#observing the number of support vectors as the parameter C increases\n",
    "for c in range(1,20,1):\n",
    "    params=\"-t 0 -c \"+str(c)\n",
    "    model = svm_train(training_set_labels,training_set_x, params)\n",
    "    print(\"C:\"+str(c)+\"\\t # of support vectors:\"+str(len(model.get_SV())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As C increases, the machine tolerates less error, which results in a narrower margin. The number of vectors in the margin understandably decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - 15 pts\n",
    "\n",
    "Please investigate the changes in the hyperplane when you remove one of the support vectors, vs., one data point that is not a support vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################\n",
      "[-1.30888701  0.52571551  1.3470139   1.37874173  1.6273681   0.16133491\n",
      "  0.13203498 -1.8521204  -0.02562698 -0.0762774   0.06906356  2.72479396\n",
      "  0.44811908] 2.3785383593196174\n",
      "##################################\n",
      "[-1.32983847  0.50137912  1.51487358  1.32368342  1.22296991  0.09252357\n",
      "  0.23434143 -1.66513531 -0.08056412  0.29650759  0.01183793  2.98157394\n",
      "  0.42209016] 2.426370193620015\n",
      "##################################\n",
      "[-1.30888701  0.52571551  1.3470139   1.37874173  1.6273681   0.16133491\n",
      "  0.13203498 -1.8521204  -0.02562698 -0.0762774   0.06906356  2.72479396\n",
      "  0.44811908] 2.3785383593196174\n"
     ]
    }
   ],
   "source": [
    "# Get support vectors as numpy arrays\n",
    "# LibSVM returns a dictionary with feature indices as keys\n",
    "# Moreover, this dictionary doesn't include feature indices\n",
    "# for feature that are 0, therefore we can not use a one liner\n",
    "# like sv=[model.get_SV()[i+1] for i in range(0,13)]\n",
    "def get_reshape_SV(m):\n",
    "    tempSupVecs=m.get_SV()\n",
    "    d=len(training_set_x[0])\n",
    "    supVecs=np.array([0.0]*(len(tempSupVecs)*d))\n",
    "    supVecs=supVecs.reshape((len(tempSupVecs),d))\n",
    "\n",
    "    for v in range(0,len(tempSupVecs)):\n",
    "        for i in range(0,d):\n",
    "            if i+1 in tempSupVecs[v].keys():\n",
    "                supVecs[v][i]=tempSupVecs[v][i+1]\n",
    "            else:\n",
    "                supVecs[v][i]=0\n",
    "    return supVecs\n",
    "\n",
    "#function getWeightsAndBias takes SVM modela and returns the weights of the support vectors and bias of the model\n",
    "def getWeightsAndBias(model):\n",
    "    SVs=get_reshape_SV(model)\n",
    "    SVcoef = np.array([coef[0] for coef in model.get_sv_coef() ])   \n",
    "    weights = np.matmul(SVs.T,SVcoef)\n",
    "    bias = -model.rho[0]\n",
    "    return weights,bias\n",
    "\n",
    "\n",
    "\n",
    "#training the base model\n",
    "params= \"-t 0 -c \" + str(c)\n",
    "model_base = svm_train(training_set_labels,training_set_x, params)\n",
    "\n",
    "\n",
    "#get the support vectos and rearrange the support vector array for further use\n",
    "SVs_base=get_reshape_SV(model_base)\n",
    "# Get index of the first support vector\n",
    "SV0=SVs_base[0]\n",
    "SV_index=-1\n",
    "nonSV_index=-1\n",
    "for i,dp in enumerate(training_set_x):\n",
    "    if np.array_equal(np.array(dp),SV0):\n",
    "        SV_index=i\n",
    "        \n",
    "# Get index of the first non support vector data point \n",
    "cont=True\n",
    "for ix,dp in enumerate(training_set_x):\n",
    "    if cont:\n",
    "        for isv,sv in enumerate(SVs_base):\n",
    "            if not np.array_equal(np.array(dp),SVs_base[isv]):\n",
    "                nonSV_index=ix\n",
    "                cont=False\n",
    "                \n",
    "#weights and bias values of the base model\n",
    "weights_base,bias_base  = getWeightsAndBias(model_base)\n",
    "\n",
    "#training model with removed support vector\n",
    "training_set_x_removedSV = np.delete(training_set_x,SV_index,axis=0)\n",
    "training_set_labels_removedSV = np.delete(training_set_labels, SV_index)\n",
    "model_removedSV = svm_train(training_set_labels_removedSV,training_set_x_removedSV, params)\n",
    "\n",
    "#weights and bias values of the base model\n",
    "weights_removedSV ,bias_removedSV  = getWeightsAndBias(model_removedSV)\n",
    "\n",
    "#training model with removed support vector\n",
    "training_set_x_removedNonSV = np.delete(training_set_x, nonSV_index,axis=0)\n",
    "training_set_labels_removedNonSV = np.delete(training_set_labels, nonSV_index)\n",
    "model_removedNonSV = svm_train(training_set_labels_removedNonSV,training_set_x_removedNonSV, params)\n",
    "\n",
    "#weights and bias values of the base model\n",
    "weights_removedNonSV ,bias_removedNonSV  = getWeightsAndBias(model_removedNonSV)\n",
    "print(\"##################################\")\n",
    "print(weights_base,bias_base)\n",
    "print(\"##################################\")\n",
    "print(weights_removedSV ,bias_removedSV )\n",
    "print(\"##################################\")\n",
    "print(weights_removedNonSV ,bias_removedNonSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task - 10 pts\n",
    "\n",
    "Use Python and [CVXOPT](http://cvxopt.org) QP solver to implement the hard margin SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00e+00  0.00e+00  0.00e+00]\n",
      "[ 0.00e+00  1.00e+00  0.00e+00]\n",
      "[ 0.00e+00  0.00e+00  1.00e+00]\n",
      "\n",
      "[-0.00e+00]\n",
      "[-0.00e+00]\n",
      "[-0.00e+00]\n",
      "\n",
      "[ 1.00e+00 -0.00e+00 -0.00e+00]\n",
      "[ 1.00e+00  2.00e+00  2.00e+00]\n",
      "[-1.00e+00 -2.00e+00 -0.00e+00]\n",
      "[-1.00e+00 -3.00e+00 -0.00e+00]\n",
      "\n",
      "[ 1.00e+00]\n",
      "[ 1.00e+00]\n",
      "[ 1.00e+00]\n",
      "[ 1.00e+00]\n",
      "\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.2653e-01 -9.7959e-01  6e+00  1e+00  6e+00\n",
      " 1:  7.5465e-03 -3.0317e-01  3e-01  2e-02  9e-02\n",
      " 2:  7.8915e-06 -4.9791e-03  5e-03  1e-10  1e-11\n",
      " 3:  7.9212e-10 -4.9790e-05  5e-05  1e-10  6e-13\n",
      " 4:  7.9213e-14 -4.9790e-07  5e-07  1e-10  1e-14\n",
      " 5:  7.9213e-18 -4.9790e-09  5e-09  1e-10  1e-16\n",
      "Optimal solution found.\n",
      "[[-4.41844496e-02]\n",
      " [ 2.23716118e-09]\n",
      " [-3.29205308e-09]]\n"
     ]
    }
   ],
   "source": [
    "from cvxopt import matrix\n",
    "from cvxopt import matrix as cvxopt_matrix\n",
    "from cvxopt import solvers as cvxopt_solvers\n",
    "\n",
    "#function QPSolver takes train data and its labels and applies the cvxopt library to solve qp problem\n",
    "def QPSolver(X,y):\n",
    "    \n",
    "    n, dim = X.shape\n",
    "    Q = np.eye(dim)\n",
    "    Q = np.insert(Q, 0, 0, axis=1)\n",
    "    Q = np.insert(Q, 0, 0, axis=0)\n",
    "    \n",
    "    p = np.zeros(dim + 1 ).reshape(-1,1)\n",
    "    \n",
    "    A = y.reshape(-1,1) * np.insert(X, 0, 1, axis=1) * 1.\n",
    "    \n",
    "    c = (np.ones(n) * 1.).reshape(-1,1)\n",
    "    P = cvxopt_matrix(Q)\n",
    "    q = cvxopt_matrix(-p)\n",
    "    G = cvxopt_matrix(-A)\n",
    "    h = cvxopt_matrix(c)\n",
    "\n",
    "    print(P)\n",
    "    print(q)\n",
    "    print(G)\n",
    "    print(h)\n",
    "    #Run qp solver for the primal problem\n",
    "    sol_t = cvxopt_solvers.qp(P, q, G, h,kktsolver='ldl', options={'kktreg':1e-10})\n",
    "    #get bias and weights\n",
    "    w_t = np.array(sol_t['x'])\n",
    "    print(w_t)\n",
    "\n",
    "    \n",
    "toy_train = np.array([[0,0],[2,2],[2,0],[3,0]])\n",
    "toy_label = np.array([-1,-1,1,1])\n",
    "QPSolver(toy_train,toy_label)\n",
    "# We have correctly constructed the needed matrices and vectors according to the course slides for \n",
    "#the primal respresentation. However, the QP solver needs an additional constraint of type Ax=b, \n",
    "#which is not derived in class. We tried giving A=0 matrix and b=0 vector.\n",
    "#This resulted in an error saying Rank(A) should not be 1. \n",
    "#There is no documentation on the restrictions on Rank(A). \n",
    "#Moreover, when we omit A and b, the solver returns wrong solutions and there is no \n",
    "#documentation on the default values of A and b. Therefore, we abandon this approach and instead \n",
    "#implement the dual representation, using this web page as a guide : \n",
    "#https://xavierbourretsicotte.github.io/SVM_implementation.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we take the website:https://xavierbourretsicotte.github.io/SVM_implementation.html\n",
    "#to implement dual solution for the toy dataset and our training dataset\n",
    "dim,n = toy_train.shape\n",
    "toy_label_n = toy_label.reshape(-1,1) * 1.\n",
    "toy_train_dash = toy_label_n * toy_train\n",
    "H_t = np.matmul(toy_train_dash , toy_train_dash.T) * 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting (Q,p,A,c) (in class)  to (P,q,G,h,A,b) to implement the format of cvxopt library\n",
    "P_t = cvxopt_matrix(H_t)\n",
    "q_t = cvxopt_matrix(-np.ones((dim, 1)))\n",
    "G_t = cvxopt_matrix(-np.eye(dim))\n",
    "h_t = cvxopt_matrix(np.zeros(dim))\n",
    "A_t = cvxopt_matrix(toy_label_n.reshape(1, -1))\n",
    "b_t = cvxopt_matrix(np.zeros(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.1633e-01 -2.1224e+00  6e+00  2e+00  2e+00\n",
      " 1: -8.5663e-01 -1.5796e+00  7e-01  2e-16  3e-16\n",
      " 2: -9.9227e-01 -1.0195e+00  3e-02  2e-16  3e-16\n",
      " 3: -9.9992e-01 -1.0002e+00  3e-04  2e-16  2e-16\n",
      " 4: -1.0000e+00 -1.0000e+00  3e-06  2e-16  3e-16\n",
      " 5: -1.0000e+00 -1.0000e+00  3e-08  2e-16  2e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "#Run cvxopt solver to solve qp problem\n",
    "sol_t = cvxopt_solvers.qp(P_t, q_t, G_t, h_t, A_t, b_t)\n",
    "#take the solution alphas\n",
    "alphas = np.array(sol_t['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "Alphas =  [0.5        0.50000001 1.        ]\n",
      "w =  [ 1.00000001 -1.00000001]\n",
      "b =  -1.0\n"
     ]
    }
   ],
   "source": [
    "yalpha=(toy_label_n* alphas)\n",
    "weights=np.matmul(yalpha.T,toy_train).reshape(toy_train.shape[1])\n",
    "# indices of nonzero alpha values\n",
    "indice_list = []\n",
    "for i in range(0,len(alphas)):\n",
    "    if alphas[i]>1e-4:\n",
    "        indice_list.append(i)\n",
    "indice_list = np.array(indice_list)\n",
    "print(indice_list)\n",
    "bias = toy_label_n[indice_list] - np.matmul(toy_train[indice_list], weights)\n",
    "\n",
    "print('Alphas = ',alphas[alphas > 1e-4])\n",
    "print('w = ', weights.flatten())\n",
    "print('b = ', bias[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
