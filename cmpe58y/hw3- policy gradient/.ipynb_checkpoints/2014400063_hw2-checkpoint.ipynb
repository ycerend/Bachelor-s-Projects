{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36332494]\n"
     ]
    }
   ],
   "source": [
    "w2 = np.random.randn(1) / np.sqrt(1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network with single layer and 1 neuron \n",
    "# a single neuron and can solve the task approximately in 50 episodes \n",
    "#where there are 50 roll- outs in each episode.\n",
    "#Considering the state space, there are 4 weights and 1 bias for the neuron. \n",
    "#Activation function is a sigmoid. Discount factor is 0.99, learning rate is 0.05. \n",
    "#Average reward of the roll-outs is used as baseline.\n",
    "\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "H = 1 # number of hidden layer neurons\n",
    "batch_size = 50 # every how many episodes to do a param update?\n",
    "learning_rate = 0.05\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 4 * 1 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "  model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "  model = {}\n",
    "  model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "  model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(x): \n",
    "  return 1.0 / (1.0 + np.exp(-x)) \n",
    "\n",
    "def run_policy(observation):\n",
    "    h = np.dot(model['W1'], observation)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self.env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "while True:\n",
    "  if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "  cur_x = prepro(observation)\n",
    "  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "  prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "  aprob, h = policy_forward(x)\n",
    "  action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "  xs.append(x) # observation\n",
    "  hs.append(h) # hidden state\n",
    "  y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "  dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  reward_sum += reward\n",
    "\n",
    "  drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "  if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    epx = np.vstack(xs)\n",
    "    eph = np.vstack(hs)\n",
    "    epdlogp = np.vstack(dlogps)\n",
    "    epr = np.vstack(drs)\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    if episode_number % batch_size == 0:\n",
    "      for k,v in model.iteritems():\n",
    "        g = grad_buffer[k] # gradient\n",
    "        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "        grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward)\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "for i in range(0,self.number_of_episodes):\n",
    "    \n",
    "            #starting a new episode via resetting and getting the first observation\n",
    "            current_observation =  env.reset()\n",
    "            \n",
    "            #resetting reward\n",
    "            numberOfStepsGetReward = 0\n",
    "            print(\"episode: \" + str(i))\n",
    "            #running an episode it will stop after 500 timestep\n",
    "            \n",
    "            for numberOfSteps in range(self.time_steps):\n",
    "                #choose an action with respect to previous observation\n",
    "                aprob, h = policy_forward(x)\n",
    "                action = 0 if np.random.uniform() < aprob else 1 # roll the dice!\n",
    "                \n",
    "                # record various intermediates (needed later for backprop)\n",
    "                xs.append(x) # observation\n",
    "                hs.append(h) # hidden state\n",
    "\n",
    "                #get current observation and reward gotten from the action that has selectted\n",
    "                new_observation,reward , _,_ = env.step(action)\n",
    "                #discretize the new observation as new state\n",
    "                new_state =self.computeStateActionValues(new_observation)\n",
    "                self.updateAvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                self.updateBvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                #make new state current state to take an action\n",
    "                current_state = new_state\n",
    "                #collect the reward, if the reward is 0 it will remain same\n",
    "                numberOfStepsGetReward += reward\n",
    "            #keep the total score of the reward\n",
    "            self.scores.append(numberOfStepsGetReward)\n",
    "            #save consecutive episodes,if the total reward is over 400\n",
    "            if numberOfStepsGetReward >= 400:\n",
    "                NumberOfConsecutiveAcceptedReward += 1\n",
    "            else:\n",
    "                NumberOfConsecutiveAcceptedReward = 0\n",
    "            #if 30 consecutive episodes that has a reward over 400 , stop\n",
    "            if NumberOfConsecutiveAcceptedReward >= 30:\n",
    "                print(\"converged\")\n",
    "                print(i)\n",
    "                break\n",
    "        #after learning plot the results\n",
    "        self.plotScores()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class QSolver simply initiates all the parameters and runs the agent. \n",
    "-tryNum method to discretize continuous values of cartpole observations. \n",
    "-updateEpsilon() and updateAlpha() functions reduces the value of epsilon and alpha over episodes to\n",
    "increase learning. \n",
    "- chooseAction() method chooses an action with respect to epsilon value as random or state that hasmaximum value on the \n",
    "Q table.\n",
    "- updateQTable() updates the q table with respect to the action that has been taken and its reward.\n",
    "-plotScores() is a helper method to plot the final reward/episode figure and saves it.\n",
    "- runEpisodes() runs the episodes and learns how to make stick stable.\n",
    "'''\n",
    "\n",
    "class QSolver():\n",
    "    def __init__(self,alpha = 0.03,epsilon = 1, gamma = 0.999,time_steps = 500,number_of_episodes=1000, position_bucket=2, velocity_bucket=2, angle_bucket = 10, ang_vel_bucket = 16):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.time_steps = time_steps\n",
    "        self.number_of_episodes = number_of_episodes\n",
    "        self.position_bucket =  position_bucket\n",
    "        self.velocity_bucket = velocity_bucket \n",
    "        self.angle_bucket = angle_bucket\n",
    "        self.ang_vel_bucket = ang_vel_bucket\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.Q = np.zeros((self.position_bucket,)+(self.velocity_bucket,)+(self.angle_bucket,)+(self.ang_vel_bucket,)+(self.env.action_space.n,))\n",
    "        self.boxes = (self.position_bucket,)+(self.velocity_bucket,)+(self.angle_bucket,)+(self.ang_vel_bucket,)\n",
    "        self.scores = []\n",
    "        self.A = np.empty(shape=(4,2))\n",
    "        self.A.fill(0)\n",
    "        self.b = np.empty(shape=(1,2))\n",
    "        self.b.fill(0)\n",
    "        self.derivative = 0\n",
    "        \n",
    "    #compute out value of the observation\n",
    "    def computeStateActionValues(self,obs):\n",
    "        out = np.matmul(obs,self.A) + self.b\n",
    "        return out\n",
    "\n",
    "    \n",
    "    #update epsilon each episode\n",
    "    #I took the equation from the website: https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "    def updateEpsilon(self,t,epsilon):\n",
    "        epsilon = epsilon * 1/(1 + 0.0001 * (t + 1))\n",
    "        return epsilon\n",
    "    \n",
    "    #update alpha each episode\n",
    "    #I took the equation from the website: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "    def updateAlpha(self,t,alpha):\n",
    "        k = 0.1\n",
    "        alpha = alpha * math.exp(-k*t)\n",
    "        return alpha\n",
    "\n",
    "    #choose an action with respect to epsilon\n",
    "    #if random value is smaller than epsilon, return a random state, to break a loop and increase the learning\n",
    "    #else return the state that has maximum reward\n",
    "    def chooseAction(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(state[0])\n",
    "    \n",
    "    #compute the first derivative of L with respect to out, dL/d(out) to update both A and b values\n",
    "    def computeDerivativeOfLwrtOut(self,reward,current_action,old_state_action_value ,old_observation,new_observation):\n",
    "        #compute out value of next observations\n",
    "        actionValueFromNewObservation = self.computeStateActionValues(new_observation)        \n",
    "        maxActionFromNewObservation = np.argmax(actionValueFromNewObservation[0])\n",
    "        #compute the derivative of L with respect to out\n",
    "        derivative= -(reward + self.gamma * actionValueFromNewObservation[0][maxActionFromNewObservation]) + old_state_action_value[0][current_action]\n",
    "        self.derivative = derivative\n",
    "        return derivative\n",
    "    \n",
    "    #update A value with respect to derivate of L and derivative of out with respect to A value\n",
    "    def updateAvalue(self,reward,current_action,old_state_action_value,old_observation,new_observation,alpha):\n",
    "        computeDerivative=self.computeDerivativeOfLwrtOut(reward,current_action,old_state_action_value,old_observation,new_observation)\n",
    "        derivative = np.matmul(computeDerivative.reshape(1,1),old_observation.reshape(1,4))\n",
    "        self.A[:,current_action] = self.A[:,current_action] - alpha * derivative\n",
    "        \n",
    "\n",
    "    #update b value with respect to derivate of L \n",
    "    def updateBvalue(self,reward,current_action,old_state_action_value,old_observation,new_observation,alpha):\n",
    "        derivative = self.computeDerivativeOfLwrtOut(reward,current_action,old_state_action_value,old_observation,new_observation)\n",
    "\n",
    "        self.b[:,current_action] = self.b[:,current_action] - alpha * derivative\n",
    "    \n",
    "    #plot episode / total reward\n",
    "    def plotScores(self):\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('total reward')\n",
    "        plt.savefig('RewardOverEpisodes.png')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "    def runEpisodes(self):\n",
    "        #initializing reward that has been taken, it will keep the total reward of an episode\n",
    "        NumberOfConsecutiveAcceptedReward = 0\n",
    "        epsilon  =self.epsilon\n",
    "        alpha = self.alpha\n",
    "        for i in range(0,self.number_of_episodes):\n",
    "            #starting a new episode via resetting and getting the first observation\n",
    "            current_observation =  self.env.reset()\n",
    "            #get current action values with respect to observation\n",
    "            current_state = self.computeStateActionValues(current_observation)\n",
    "            #update epsilon and alpha values for the episode \n",
    "            epsilon = self.updateEpsilon(i,epsilon)\n",
    "            alpha = self.updateAlpha(i,alpha)\n",
    "            #resetting reward\n",
    "            numberOfStepsGetReward = 0\n",
    "            print(\"episode: \" + str(i))\n",
    "            #running an episode it will stop after 500 timestep\n",
    "            for numberOfSteps in range(self.time_steps):\n",
    "                #choose an action with respect to previous observation\n",
    "                action = self.chooseAction(current_state,epsilon)\n",
    "                #get current observation and reward gotten from the action that has selectted\n",
    "                new_observation,reward , _,_ = self.env.step(action)\n",
    "                #discretize the new observation as new state\n",
    "                new_state =self.computeStateActionValues(new_observation)\n",
    "                self.updateAvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                self.updateBvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                #make new state current state to take an action\n",
    "                current_state = new_state\n",
    "                #collect the reward, if the reward is 0 it will remain same\n",
    "                numberOfStepsGetReward += reward\n",
    "            #keep the total score of the reward\n",
    "            self.scores.append(numberOfStepsGetReward)\n",
    "            #save consecutive episodes,if the total reward is over 400\n",
    "            if numberOfStepsGetReward >= 400:\n",
    "                NumberOfConsecutiveAcceptedReward += 1\n",
    "            else:\n",
    "                NumberOfConsecutiveAcceptedReward = 0\n",
    "            #if 30 consecutive episodes that has a reward over 400 , stop\n",
    "            if NumberOfConsecutiveAcceptedReward >= 30:\n",
    "                print(\"converged\")\n",
    "                print(i)\n",
    "                break\n",
    "        #after learning plot the results\n",
    "        self.plotScores()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "episode: 1\n",
      "episode: 2\n",
      "episode: 3\n",
      "episode: 4\n",
      "episode: 5\n",
      "episode: 6\n",
      "episode: 7\n",
      "episode: 8\n",
      "episode: 9\n",
      "episode: 10\n",
      "episode: 11\n",
      "episode: 12\n",
      "episode: 13\n",
      "episode: 14\n",
      "episode: 15\n",
      "episode: 16\n",
      "episode: 17\n",
      "episode: 18\n",
      "episode: 19\n",
      "episode: 20\n",
      "episode: 21\n",
      "episode: 22\n",
      "episode: 23\n",
      "episode: 24\n",
      "episode: 25\n",
      "episode: 26\n",
      "episode: 27\n",
      "episode: 28\n",
      "episode: 29\n",
      "episode: 30\n",
      "episode: 31\n",
      "episode: 32\n",
      "episode: 33\n",
      "episode: 34\n",
      "episode: 35\n",
      "episode: 36\n",
      "episode: 37\n",
      "episode: 38\n",
      "episode: 39\n",
      "episode: 40\n",
      "episode: 41\n",
      "episode: 42\n",
      "episode: 43\n",
      "episode: 44\n",
      "episode: 45\n",
      "episode: 46\n",
      "episode: 47\n",
      "episode: 48\n",
      "episode: 49\n",
      "episode: 50\n",
      "episode: 51\n",
      "episode: 52\n",
      "episode: 53\n",
      "episode: 54\n",
      "episode: 55\n",
      "episode: 56\n",
      "episode: 57\n",
      "episode: 58\n",
      "episode: 59\n",
      "episode: 60\n",
      "episode: 61\n",
      "episode: 62\n",
      "episode: 63\n",
      "episode: 64\n",
      "episode: 65\n",
      "episode: 66\n",
      "episode: 67\n",
      "episode: 68\n",
      "episode: 69\n",
      "episode: 70\n",
      "episode: 71\n",
      "episode: 72\n",
      "episode: 73\n",
      "episode: 74\n",
      "episode: 75\n",
      "episode: 76\n",
      "episode: 77\n",
      "episode: 78\n",
      "episode: 79\n",
      "episode: 80\n",
      "episode: 81\n",
      "episode: 82\n",
      "episode: 83\n",
      "episode: 84\n",
      "episode: 85\n",
      "episode: 86\n",
      "episode: 87\n",
      "episode: 88\n",
      "episode: 89\n",
      "episode: 90\n",
      "episode: 91\n",
      "episode: 92\n",
      "episode: 93\n",
      "episode: 94\n",
      "episode: 95\n",
      "episode: 96\n",
      "episode: 97\n",
      "episode: 98\n",
      "episode: 99\n",
      "episode: 100\n",
      "episode: 101\n",
      "episode: 102\n",
      "episode: 103\n",
      "episode: 104\n",
      "episode: 105\n",
      "episode: 106\n",
      "episode: 107\n",
      "episode: 108\n",
      "episode: 109\n",
      "episode: 110\n",
      "episode: 111\n",
      "episode: 112\n",
      "episode: 113\n",
      "episode: 114\n",
      "episode: 115\n",
      "episode: 116\n",
      "episode: 117\n",
      "episode: 118\n",
      "episode: 119\n",
      "episode: 120\n",
      "episode: 121\n",
      "episode: 122\n",
      "episode: 123\n",
      "episode: 124\n",
      "episode: 125\n",
      "episode: 126\n",
      "episode: 127\n",
      "episode: 128\n",
      "episode: 129\n",
      "episode: 130\n",
      "episode: 131\n",
      "episode: 132\n",
      "episode: 133\n",
      "episode: 134\n",
      "episode: 135\n",
      "episode: 136\n",
      "episode: 137\n",
      "episode: 138\n",
      "episode: 139\n",
      "episode: 140\n",
      "episode: 141\n",
      "episode: 142\n",
      "episode: 143\n",
      "episode: 144\n",
      "episode: 145\n",
      "episode: 146\n",
      "episode: 147\n",
      "episode: 148\n",
      "episode: 149\n",
      "episode: 150\n",
      "episode: 151\n",
      "episode: 152\n",
      "episode: 153\n",
      "episode: 154\n",
      "episode: 155\n",
      "episode: 156\n",
      "episode: 157\n",
      "episode: 158\n",
      "episode: 159\n",
      "episode: 160\n",
      "episode: 161\n",
      "episode: 162\n",
      "episode: 163\n",
      "episode: 164\n",
      "episode: 165\n",
      "episode: 166\n",
      "episode: 167\n",
      "episode: 168\n",
      "episode: 169\n",
      "episode: 170\n",
      "episode: 171\n",
      "episode: 172\n",
      "episode: 173\n",
      "episode: 174\n",
      "episode: 175\n",
      "episode: 176\n",
      "episode: 177\n",
      "episode: 178\n",
      "episode: 179\n",
      "episode: 180\n",
      "episode: 181\n",
      "episode: 182\n",
      "episode: 183\n",
      "episode: 184\n",
      "episode: 185\n",
      "episode: 186\n",
      "episode: 187\n",
      "episode: 188\n",
      "episode: 189\n",
      "episode: 190\n",
      "episode: 191\n",
      "episode: 192\n",
      "episode: 193\n",
      "episode: 194\n",
      "episode: 195\n",
      "episode: 196\n",
      "episode: 197\n",
      "episode: 198\n",
      "episode: 199\n",
      "episode: 200\n",
      "episode: 201\n",
      "episode: 202\n",
      "episode: 203\n",
      "episode: 204\n",
      "episode: 205\n",
      "episode: 206\n",
      "episode: 207\n",
      "episode: 208\n",
      "episode: 209\n",
      "episode: 210\n",
      "episode: 211\n",
      "episode: 212\n",
      "episode: 213\n",
      "episode: 214\n",
      "episode: 215\n",
      "episode: 216\n",
      "episode: 217\n",
      "episode: 218\n",
      "episode: 219\n",
      "episode: 220\n",
      "episode: 221\n",
      "episode: 222\n",
      "episode: 223\n",
      "episode: 224\n",
      "episode: 225\n",
      "episode: 226\n",
      "episode: 227\n",
      "episode: 228\n",
      "episode: 229\n",
      "episode: 230\n",
      "episode: 231\n",
      "episode: 232\n",
      "episode: 233\n",
      "episode: 234\n",
      "episode: 235\n",
      "episode: 236\n",
      "episode: 237\n",
      "episode: 238\n",
      "episode: 239\n",
      "episode: 240\n",
      "episode: 241\n",
      "episode: 242\n",
      "episode: 243\n",
      "episode: 244\n",
      "episode: 245\n",
      "episode: 246\n",
      "episode: 247\n",
      "episode: 248\n",
      "episode: 249\n",
      "episode: 250\n",
      "episode: 251\n",
      "episode: 252\n",
      "episode: 253\n",
      "episode: 254\n",
      "episode: 255\n",
      "episode: 256\n",
      "episode: 257\n",
      "episode: 258\n",
      "episode: 259\n",
      "episode: 260\n",
      "episode: 261\n",
      "episode: 262\n",
      "episode: 263\n",
      "episode: 264\n",
      "episode: 265\n",
      "episode: 266\n",
      "episode: 267\n",
      "episode: 268\n",
      "episode: 269\n",
      "episode: 270\n",
      "episode: 271\n",
      "episode: 272\n",
      "episode: 273\n",
      "episode: 274\n",
      "episode: 275\n",
      "episode: 276\n",
      "episode: 277\n",
      "episode: 278\n",
      "episode: 279\n",
      "episode: 280\n",
      "episode: 281\n",
      "episode: 282\n",
      "episode: 283\n",
      "episode: 284\n",
      "episode: 285\n",
      "episode: 286\n",
      "episode: 287\n",
      "episode: 288\n",
      "episode: 289\n",
      "episode: 290\n",
      "episode: 291\n",
      "episode: 292\n",
      "episode: 293\n",
      "episode: 294\n",
      "episode: 295\n",
      "episode: 296\n",
      "episode: 297\n",
      "episode: 298\n",
      "episode: 299\n",
      "episode: 300\n",
      "episode: 301\n",
      "episode: 302\n",
      "episode: 303\n",
      "episode: 304\n",
      "episode: 305\n",
      "episode: 306\n",
      "episode: 307\n",
      "episode: 308\n",
      "episode: 309\n",
      "episode: 310\n",
      "episode: 311\n",
      "episode: 312\n",
      "episode: 313\n",
      "episode: 314\n",
      "episode: 315\n",
      "episode: 316\n",
      "episode: 317\n",
      "episode: 318\n",
      "episode: 319\n",
      "episode: 320\n",
      "episode: 321\n",
      "episode: 322\n",
      "episode: 323\n",
      "episode: 324\n",
      "episode: 325\n",
      "episode: 326\n",
      "episode: 327\n",
      "episode: 328\n",
      "episode: 329\n",
      "episode: 330\n",
      "episode: 331\n",
      "episode: 332\n",
      "episode: 333\n",
      "episode: 334\n",
      "episode: 335\n",
      "episode: 336\n",
      "episode: 337\n",
      "episode: 338\n",
      "episode: 339\n",
      "episode: 340\n",
      "episode: 341\n",
      "episode: 342\n",
      "episode: 343\n",
      "episode: 344\n",
      "episode: 345\n",
      "episode: 346\n",
      "episode: 347\n",
      "episode: 348\n",
      "episode: 349\n",
      "episode: 350\n",
      "episode: 351\n",
      "episode: 352\n",
      "episode: 353\n",
      "episode: 354\n",
      "episode: 355\n",
      "episode: 356\n",
      "episode: 357\n",
      "episode: 358\n",
      "episode: 359\n",
      "episode: 360\n",
      "episode: 361\n",
      "episode: 362\n",
      "episode: 363\n",
      "episode: 364\n",
      "episode: 365\n",
      "episode: 366\n",
      "episode: 367\n",
      "episode: 368\n",
      "episode: 369\n",
      "episode: 370\n",
      "episode: 371\n",
      "episode: 372\n",
      "episode: 373\n",
      "episode: 374\n",
      "episode: 375\n",
      "episode: 376\n",
      "episode: 377\n",
      "episode: 378\n",
      "episode: 379\n",
      "episode: 380\n",
      "episode: 381\n",
      "episode: 382\n",
      "episode: 383\n",
      "episode: 384\n",
      "episode: 385\n",
      "episode: 386\n",
      "episode: 387\n",
      "episode: 388\n",
      "episode: 389\n",
      "episode: 390\n",
      "episode: 391\n",
      "episode: 392\n",
      "episode: 393\n",
      "episode: 394\n",
      "episode: 395\n",
      "episode: 396\n",
      "episode: 397\n",
      "episode: 398\n",
      "episode: 399\n",
      "episode: 400\n",
      "episode: 401\n",
      "episode: 402\n",
      "episode: 403\n",
      "episode: 404\n",
      "episode: 405\n",
      "episode: 406\n",
      "episode: 407\n",
      "episode: 408\n",
      "episode: 409\n",
      "episode: 410\n",
      "episode: 411\n",
      "episode: 412\n",
      "episode: 413\n",
      "episode: 414\n",
      "episode: 415\n",
      "episode: 416\n",
      "episode: 417\n",
      "episode: 418\n",
      "episode: 419\n",
      "episode: 420\n",
      "episode: 421\n",
      "episode: 422\n",
      "episode: 423\n",
      "episode: 424\n",
      "episode: 425\n",
      "episode: 426\n",
      "episode: 427\n",
      "episode: 428\n",
      "episode: 429\n",
      "episode: 430\n",
      "episode: 431\n",
      "episode: 432\n",
      "episode: 433\n",
      "episode: 434\n",
      "episode: 435\n",
      "episode: 436\n",
      "episode: 437\n",
      "episode: 438\n",
      "episode: 439\n",
      "episode: 440\n",
      "episode: 441\n",
      "episode: 442\n",
      "episode: 443\n",
      "episode: 444\n",
      "episode: 445\n",
      "episode: 446\n",
      "episode: 447\n",
      "episode: 448\n",
      "episode: 449\n",
      "episode: 450\n",
      "episode: 451\n",
      "episode: 452\n",
      "episode: 453\n",
      "episode: 454\n",
      "episode: 455\n",
      "episode: 456\n",
      "episode: 457\n",
      "episode: 458\n",
      "episode: 459\n",
      "episode: 460\n",
      "episode: 461\n",
      "episode: 462\n",
      "episode: 463\n",
      "episode: 464\n",
      "episode: 465\n",
      "episode: 466\n",
      "episode: 467\n",
      "episode: 468\n",
      "episode: 469\n",
      "episode: 470\n",
      "episode: 471\n",
      "episode: 472\n",
      "episode: 473\n",
      "episode: 474\n",
      "episode: 475\n",
      "episode: 476\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # we make an agent to run the episode\n",
    "    agent = QSolver()\n",
    "    #then we simply run the agent\n",
    "    agent.runEpisodes()\n",
    "    #agent.deneme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
