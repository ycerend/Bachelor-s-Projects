{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(8).reshape(2,2,2) \n",
    "b = np.arange(8)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class QSolver simply initiates all the parameters and runs the agent. \n",
    "-tryNum method to discretize continuous values of cartpole observations. \n",
    "-updateEpsilon() and updateAlpha() functions reduces the value of epsilon and alpha over episodes to\n",
    "increase learning. \n",
    "- chooseAction() method chooses an action with respect to epsilon value as random or state that hasmaximum value on the \n",
    "Q table.\n",
    "- updateQTable() updates the q table with respect to the action that has been taken and its reward.\n",
    "-plotScores() is a helper method to plot the final reward/episode figure and saves it.\n",
    "- runEpisodes() runs the episodes and learns how to make stick stable.\n",
    "'''\n",
    "\n",
    "class QSolver():\n",
    "    def __init__(self,alpha = 0.03,epsilon = 1, gamma = 0.999,time_steps = 500,number_of_episodes=1000, position_bucket=2, velocity_bucket=2, angle_bucket = 10, ang_vel_bucket = 16):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.time_steps = time_steps\n",
    "        self.number_of_episodes = number_of_episodes\n",
    "        self.position_bucket =  position_bucket\n",
    "        self.velocity_bucket = velocity_bucket \n",
    "        self.angle_bucket = angle_bucket\n",
    "        self.ang_vel_bucket = ang_vel_bucket\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self.Q = np.zeros((self.position_bucket,)+(self.velocity_bucket,)+(self.angle_bucket,)+(self.ang_vel_bucket,)+(self.env.action_space.n,))\n",
    "        self.boxes = (self.position_bucket,)+(self.velocity_bucket,)+(self.angle_bucket,)+(self.ang_vel_bucket,)\n",
    "        self.scores = []\n",
    "        self.A = np.empty(shape=(4,2))\n",
    "        self.A.fill(0)\n",
    "        self.b = np.empty(shape=(1,2))\n",
    "        self.b.fill(0)\n",
    "        self.derivative = 0\n",
    "        \n",
    "    #compute out value of the observation\n",
    "    def computeStateActionValues(self,obs):\n",
    "        out = np.matmul(obs,self.A) + self.b\n",
    "        return out\n",
    "\n",
    "    \n",
    "    #update epsilon each episode\n",
    "    #I took the equation from the website: https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
    "    def updateEpsilon(self,t,epsilon):\n",
    "        epsilon = epsilon * 1/(1 + 0.0001 * (t + 1))\n",
    "        return epsilon\n",
    "    \n",
    "    #update alpha each episode\n",
    "    #I took the equation from the website: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "    def updateAlpha(self,t,alpha):\n",
    "        k = 0.1\n",
    "        alpha = alpha * math.exp(-k*t)\n",
    "        return alpha\n",
    "\n",
    "    #choose an action with respect to epsilon\n",
    "    #if random value is smaller than epsilon, return a random state, to break a loop and increase the learning\n",
    "    #else return the state that has maximum reward\n",
    "    def chooseAction(self, state, epsilon):\n",
    "        if np.random.random() <= epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(state[0])\n",
    "    \n",
    "    #compute the first derivative of L with respect to out, dL/d(out) to update both A and b values\n",
    "    def computeDerivativeOfLwrtOut(self,reward,current_action,old_state_action_value ,old_observation,new_observation):\n",
    "        #compute out value of next observations\n",
    "        actionValueFromNewObservation = self.computeStateActionValues(new_observation)        \n",
    "        maxActionFromNewObservation = np.argmax(actionValueFromNewObservation[0])\n",
    "        #compute the derivative of L with respect to out\n",
    "        derivative= -(reward + self.gamma * actionValueFromNewObservation[0][maxActionFromNewObservation]) + old_state_action_value[0][current_action]\n",
    "        self.derivative = derivative\n",
    "        return derivative\n",
    "    \n",
    "    #update A value with respect to derivate of L and derivative of out with respect to A value\n",
    "    def updateAvalue(self,reward,current_action,old_state_action_value,old_observation,new_observation,alpha):\n",
    "        computeDerivative=self.computeDerivativeOfLwrtOut(reward,current_action,old_state_action_value,old_observation,new_observation)\n",
    "        derivative = np.matmul(computeDerivative.reshape(1,1),old_observation.reshape(1,4))\n",
    "        self.A[:,current_action] = self.A[:,current_action] - alpha * derivative\n",
    "        \n",
    "\n",
    "    #update b value with respect to derivate of L \n",
    "    def updateBvalue(self,reward,current_action,old_state_action_value,old_observation,new_observation,alpha):\n",
    "        derivative = self.computeDerivativeOfLwrtOut(reward,current_action,old_state_action_value,old_observation,new_observation)\n",
    "\n",
    "        self.b[:,current_action] = self.b[:,current_action] - alpha * derivative\n",
    "    \n",
    "    #plot episode / total reward\n",
    "    def plotScores(self):\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel('episode')\n",
    "        plt.ylabel('total reward')\n",
    "        plt.savefig('RewardOverEpisodes.png')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "    def runEpisodes(self):\n",
    "        #initializing reward that has been taken, it will keep the total reward of an episode\n",
    "        NumberOfConsecutiveAcceptedReward = 0\n",
    "        epsilon  =self.epsilon\n",
    "        alpha = self.alpha\n",
    "        for i in range(0,self.number_of_episodes):\n",
    "            #starting a new episode via resetting and getting the first observation\n",
    "            current_observation =  self.env.reset()\n",
    "            #get current action values with respect to observation\n",
    "            current_state = self.computeStateActionValues(current_observation)\n",
    "            #update epsilon and alpha values for the episode \n",
    "            epsilon = self.updateEpsilon(i,epsilon)\n",
    "            alpha = self.updateAlpha(i,alpha)\n",
    "            #resetting reward\n",
    "            numberOfStepsGetReward = 0\n",
    "            print(\"episode: \" + str(i))\n",
    "            #running an episode it will stop after 500 timestep\n",
    "            for numberOfSteps in range(self.time_steps):\n",
    "                #choose an action with respect to previous observation\n",
    "                action = self.chooseAction(current_state,epsilon)\n",
    "                #get current observation and reward gotten from the action that has selectted\n",
    "                new_observation,reward , _,_ = self.env.step(action)\n",
    "                #discretize the new observation as new state\n",
    "                new_state =self.computeStateActionValues(new_observation)\n",
    "                self.updateAvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                self.updateBvalue(reward,action,current_state,current_observation,new_observation,alpha)\n",
    "                #make new state current state to take an action\n",
    "                current_state = new_state\n",
    "                #collect the reward, if the reward is 0 it will remain same\n",
    "                numberOfStepsGetReward += reward\n",
    "            #keep the total score of the reward\n",
    "            self.scores.append(numberOfStepsGetReward)\n",
    "            #save consecutive episodes,if the total reward is over 400\n",
    "            if numberOfStepsGetReward >= 400:\n",
    "                NumberOfConsecutiveAcceptedReward += 1\n",
    "            else:\n",
    "                NumberOfConsecutiveAcceptedReward = 0\n",
    "            #if 30 consecutive episodes that has a reward over 400 , stop\n",
    "            if NumberOfConsecutiveAcceptedReward >= 30:\n",
    "                print(\"converged\")\n",
    "                print(i)\n",
    "                break\n",
    "        #after learning plot the results\n",
    "        self.plotScores()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n",
      "episode: 1\n",
      "episode: 2\n",
      "episode: 3\n",
      "episode: 4\n",
      "episode: 5\n",
      "episode: 6\n",
      "episode: 7\n",
      "episode: 8\n",
      "episode: 9\n",
      "episode: 10\n",
      "episode: 11\n",
      "episode: 12\n",
      "episode: 13\n",
      "episode: 14\n",
      "episode: 15\n",
      "episode: 16\n",
      "episode: 17\n",
      "episode: 18\n",
      "episode: 19\n",
      "episode: 20\n",
      "episode: 21\n",
      "episode: 22\n",
      "episode: 23\n",
      "episode: 24\n",
      "episode: 25\n",
      "episode: 26\n",
      "episode: 27\n",
      "episode: 28\n",
      "episode: 29\n",
      "episode: 30\n",
      "episode: 31\n",
      "episode: 32\n",
      "episode: 33\n",
      "episode: 34\n",
      "episode: 35\n",
      "episode: 36\n",
      "episode: 37\n",
      "episode: 38\n",
      "episode: 39\n",
      "episode: 40\n",
      "episode: 41\n",
      "episode: 42\n",
      "episode: 43\n",
      "episode: 44\n",
      "episode: 45\n",
      "episode: 46\n",
      "episode: 47\n",
      "episode: 48\n",
      "episode: 49\n",
      "episode: 50\n",
      "episode: 51\n",
      "episode: 52\n",
      "episode: 53\n",
      "episode: 54\n",
      "episode: 55\n",
      "episode: 56\n",
      "episode: 57\n",
      "episode: 58\n",
      "episode: 59\n",
      "episode: 60\n",
      "episode: 61\n",
      "episode: 62\n",
      "episode: 63\n",
      "episode: 64\n",
      "episode: 65\n",
      "episode: 66\n",
      "episode: 67\n",
      "episode: 68\n",
      "episode: 69\n",
      "episode: 70\n",
      "episode: 71\n",
      "episode: 72\n",
      "episode: 73\n",
      "episode: 74\n",
      "episode: 75\n",
      "episode: 76\n",
      "episode: 77\n",
      "episode: 78\n",
      "episode: 79\n",
      "episode: 80\n",
      "episode: 81\n",
      "episode: 82\n",
      "episode: 83\n",
      "episode: 84\n",
      "episode: 85\n",
      "episode: 86\n",
      "episode: 87\n",
      "episode: 88\n",
      "episode: 89\n",
      "episode: 90\n",
      "episode: 91\n",
      "episode: 92\n",
      "episode: 93\n",
      "episode: 94\n",
      "episode: 95\n",
      "episode: 96\n",
      "episode: 97\n",
      "episode: 98\n",
      "episode: 99\n",
      "episode: 100\n",
      "episode: 101\n",
      "episode: 102\n",
      "episode: 103\n",
      "episode: 104\n",
      "episode: 105\n",
      "episode: 106\n",
      "episode: 107\n",
      "episode: 108\n",
      "episode: 109\n",
      "episode: 110\n",
      "episode: 111\n",
      "episode: 112\n",
      "episode: 113\n",
      "episode: 114\n",
      "episode: 115\n",
      "episode: 116\n",
      "episode: 117\n",
      "episode: 118\n",
      "episode: 119\n",
      "episode: 120\n",
      "episode: 121\n",
      "episode: 122\n",
      "episode: 123\n",
      "episode: 124\n",
      "episode: 125\n",
      "episode: 126\n",
      "episode: 127\n",
      "episode: 128\n",
      "episode: 129\n",
      "episode: 130\n",
      "episode: 131\n",
      "episode: 132\n",
      "episode: 133\n",
      "episode: 134\n",
      "episode: 135\n",
      "episode: 136\n",
      "episode: 137\n",
      "episode: 138\n",
      "episode: 139\n",
      "episode: 140\n",
      "episode: 141\n",
      "episode: 142\n",
      "episode: 143\n",
      "episode: 144\n",
      "episode: 145\n",
      "episode: 146\n",
      "episode: 147\n",
      "episode: 148\n",
      "episode: 149\n",
      "episode: 150\n",
      "episode: 151\n",
      "episode: 152\n",
      "episode: 153\n",
      "episode: 154\n",
      "episode: 155\n",
      "episode: 156\n",
      "episode: 157\n",
      "episode: 158\n",
      "episode: 159\n",
      "episode: 160\n",
      "episode: 161\n",
      "episode: 162\n",
      "episode: 163\n",
      "episode: 164\n",
      "episode: 165\n",
      "episode: 166\n",
      "episode: 167\n",
      "episode: 168\n",
      "episode: 169\n",
      "episode: 170\n",
      "episode: 171\n",
      "episode: 172\n",
      "episode: 173\n",
      "episode: 174\n",
      "episode: 175\n",
      "episode: 176\n",
      "episode: 177\n",
      "episode: 178\n",
      "episode: 179\n",
      "episode: 180\n",
      "episode: 181\n",
      "episode: 182\n",
      "episode: 183\n",
      "episode: 184\n",
      "episode: 185\n",
      "episode: 186\n",
      "episode: 187\n",
      "episode: 188\n",
      "episode: 189\n",
      "episode: 190\n",
      "episode: 191\n",
      "episode: 192\n",
      "episode: 193\n",
      "episode: 194\n",
      "episode: 195\n",
      "episode: 196\n",
      "episode: 197\n",
      "episode: 198\n",
      "episode: 199\n",
      "episode: 200\n",
      "episode: 201\n",
      "episode: 202\n",
      "episode: 203\n",
      "episode: 204\n",
      "episode: 205\n",
      "episode: 206\n",
      "episode: 207\n",
      "episode: 208\n",
      "episode: 209\n",
      "episode: 210\n",
      "episode: 211\n",
      "episode: 212\n",
      "episode: 213\n",
      "episode: 214\n",
      "episode: 215\n",
      "episode: 216\n",
      "episode: 217\n",
      "episode: 218\n",
      "episode: 219\n",
      "episode: 220\n",
      "episode: 221\n",
      "episode: 222\n",
      "episode: 223\n",
      "episode: 224\n",
      "episode: 225\n",
      "episode: 226\n",
      "episode: 227\n",
      "episode: 228\n",
      "episode: 229\n",
      "episode: 230\n",
      "episode: 231\n",
      "episode: 232\n",
      "episode: 233\n",
      "episode: 234\n",
      "episode: 235\n",
      "episode: 236\n",
      "episode: 237\n",
      "episode: 238\n",
      "episode: 239\n",
      "episode: 240\n",
      "episode: 241\n",
      "episode: 242\n",
      "episode: 243\n",
      "episode: 244\n",
      "episode: 245\n",
      "episode: 246\n",
      "episode: 247\n",
      "episode: 248\n",
      "episode: 249\n",
      "episode: 250\n",
      "episode: 251\n",
      "episode: 252\n",
      "episode: 253\n",
      "episode: 254\n",
      "episode: 255\n",
      "episode: 256\n",
      "episode: 257\n",
      "episode: 258\n",
      "episode: 259\n",
      "episode: 260\n",
      "episode: 261\n",
      "episode: 262\n",
      "episode: 263\n",
      "episode: 264\n",
      "episode: 265\n",
      "episode: 266\n",
      "episode: 267\n",
      "episode: 268\n",
      "episode: 269\n",
      "episode: 270\n",
      "episode: 271\n",
      "episode: 272\n",
      "episode: 273\n",
      "episode: 274\n",
      "episode: 275\n",
      "episode: 276\n",
      "episode: 277\n",
      "episode: 278\n",
      "episode: 279\n",
      "episode: 280\n",
      "episode: 281\n",
      "episode: 282\n",
      "episode: 283\n",
      "episode: 284\n",
      "episode: 285\n",
      "episode: 286\n",
      "episode: 287\n",
      "episode: 288\n",
      "episode: 289\n",
      "episode: 290\n",
      "episode: 291\n",
      "episode: 292\n",
      "episode: 293\n",
      "episode: 294\n",
      "episode: 295\n",
      "episode: 296\n",
      "episode: 297\n",
      "episode: 298\n",
      "episode: 299\n",
      "episode: 300\n",
      "episode: 301\n",
      "episode: 302\n",
      "episode: 303\n",
      "episode: 304\n",
      "episode: 305\n",
      "episode: 306\n",
      "episode: 307\n",
      "episode: 308\n",
      "episode: 309\n",
      "episode: 310\n",
      "episode: 311\n",
      "episode: 312\n",
      "episode: 313\n",
      "episode: 314\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # we make an agent to run the episode\n",
    "    agent = QSolver()\n",
    "    #then we simply run the agent\n",
    "    agent.runEpisodes()\n",
    "    #agent.deneme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def deneme(self):\n",
    "        current_observation =  self.env.reset()\n",
    "        current_state = self.computeStateActionValues(current_observation)\n",
    "        #print(current_state)\n",
    "        #print(type(current_state))\n",
    "        epsilon = self.updateEpsilon(0)\n",
    "        #print(self.chooseAction(current_state,epsilon))\n",
    "        action = self.chooseAction(current_state,epsilon)\n",
    "        new_observation,reward , _,_ = self.env.step(action)\n",
    "        current_state2 = self.computeStateActionValues(new_observation)\n",
    "        #print(current_state2)\n",
    "        #print(self.chooseAction(current_state,epsilon))\n",
    "        #print(new_observation)\n",
    "        #print(reward)\n",
    "        #print(\" A old\")\n",
    "        print(self.A)\n",
    "        self.updateAvalue(reward,action,current_state,current_observation,new_observation)\n",
    "        #print(\" A new\")\n",
    "        print(self.A)        \n",
    "        print(\" b old\")\n",
    "        print(self.b)\n",
    "        self.updateBvalue(reward,action,current_state,current_observation,new_observation)\n",
    "        print(\" b new\")\n",
    "        print(self.b)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
