{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 6, 12, 2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class CartPole():\n",
    "    def __init__(self, buckets=(1, 1, 6, 12,), n_episodes=1000, n_win_ticks=195, min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=25, max_env_steps=None, monitor=False):\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.n_episodes = n_episodes # training episodes \n",
    "        self.n_win_ticks = n_win_ticks # average ticks over 100 episodes required for win\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # only for development purposes\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, 'tmp/cartpole-1', force=True) # record results for upload\n",
    "\n",
    "        # initialising Q-table\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "        #print(self.buckets + (self.env.action_space.n,))\n",
    "        print(self.Q.shape)\n",
    "    # Discretizing input space to make Q-table and to reduce dimmensionality\n",
    "    def discretize(self, obs):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        print(upper_bounds)\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        print(lower_bounds)\n",
    "        ratios = [(obs[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(obs))]\n",
    "        print(ratios)\n",
    "        new_obs = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(obs))]\n",
    "        print(new_obs)\n",
    "        new_obs = [min(self.buckets[i] - 1, max(0, new_obs[i])) for i in range(len(obs))]\n",
    "        print(new_obs)\n",
    "        return tuple(new_obs)\n",
    "\n",
    "    # Choosing action based on epsilon-greedy policy\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    # Updating Q-value of state-action pair based on the update equation\n",
    "    def update_q(self, state_old, action, reward, state_new, alpha):\n",
    "        self.Q[state_old][action] += alpha * (reward + self.gamma * np.max(self.Q[state_new]) - self.Q[state_old][action])\n",
    "\n",
    "    # Adaptive learning of Exploration Rate\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    # Adaptive learning of Learning Rate\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            # As states are continuous, discretize them into buckets\n",
    "            current_state = self.discretize(self.env.reset())\n",
    "\n",
    "            # Get adaptive learning alpha and epsilon decayed over time\n",
    "            alpha = self.get_alpha(e)\n",
    "            epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            i = 0\n",
    "            final_reward = 0\n",
    "            while not done:\n",
    "                # Render environment\n",
    "                #self.env.render()\n",
    "\n",
    "                # Choose action according to greedy policy and take it\n",
    "                action = self.choose_action(current_state, epsilon)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                print(\"obs: \")\n",
    "                print(obs)\n",
    "                print(\"alll: \")\n",
    "                new_state = self.discretize(obs)\n",
    "                print(\"dis: \")\n",
    "                print(new_state)\n",
    "\n",
    "                # Update Q-Table\n",
    "                self.update_q(current_state, action, reward, new_state, alpha)\n",
    "                current_state = new_state\n",
    "                i += 1\n",
    "                final_reward = reward\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Make an instance of CartPole class \n",
    "    solver = CartPole()\n",
    "    #solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
